# ğŸš€ Plano de ImplementaÃ§Ã£o - WebQuizMedicina
## Sistema RAG Robusto com Embeddings e Batching Inteligente

---

## ğŸ“Š **Estado Atual**

### âœ… **Implementado (Phase 0 e 1)**
- âœ… Truncamento bÃ¡sico de input (3 PDFs, 40k chars)
- âœ… Batching bÃ¡sico para flashcards e quiz
- âœ… ValidaÃ§Ã£o robusta de resposta Gemini API
- âœ… Logs de debug para tamanho de prompt
- âœ… Tratamento de erros MAX_TOKENS, SAFETY, RECITATION

### âŒ **Problemas Identificados**
- âŒ **Input nÃ£o otimizado**: Truncamento bruto perde contexto relevante
- âŒ **RAG primitivo**: ConcatenaÃ§Ã£o simples, sem busca semÃ¢ntica
- âŒ **Batching reativo**: SÃ³ limita apÃ³s erro, nÃ£o previne
- âŒ **Sem embeddings**: NÃ£o hÃ¡ busca por relevÃ¢ncia
- âŒ **5+ PDFs falham**: Mesmo com 3 PDFs, conteÃºdo pode ser muito grande

---

## ğŸ¯ **Objetivo Final**

Transformar o WebQuizMedicina em uma aplicaÃ§Ã£o **sÃ³lida e escalÃ¡vel** com:

1. **RAG SemÃ¢ntico** - Busca inteligente com embeddings
2. **Batching Preventivo** - Regras que impedem erros antes de ocorrer
3. **Input Otimizado** - Apenas conteÃºdo relevante enviado ao LLM
4. **Output ConfiÃ¡vel** - Resposta sempre completa, nunca truncada
5. **Monitoramento** - Logs e mÃ©tricas para debugging

---

## ğŸ“‹ **FASE 2: RAG SemÃ¢ntico com Embeddings**

### **2.1 - Arquitetura de Embeddings**

#### **Objetivo**
Substituir concatenaÃ§Ã£o bruta por busca semÃ¢ntica inteligente usando Gemini Embeddings API.

#### **Como Funciona**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     UPLOAD DE PDF                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
                     â–¼
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚  1. Extrair Texto    â”‚
         â”‚     (parse-pdf)      â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â”‚
                    â–¼
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚  2. Chunking         â”‚
         â”‚  â€¢ 800 tokens/chunk  â”‚
         â”‚  â€¢ Overlap 100 tokensâ”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â”‚
                    â–¼
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚  3. Generate         â”‚
         â”‚     Embeddings       â”‚
         â”‚  (Gemini API)        â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â”‚
                    â–¼
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚  4. Store Vector DB  â”‚
         â”‚  (Supabase pgvector) â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### **Schema de Banco de Dados**

```sql
-- Nova tabela: source_chunks
CREATE TABLE source_chunks (
  id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
  source_id UUID REFERENCES sources(id) ON DELETE CASCADE,
  chunk_index INT NOT NULL,
  content TEXT NOT NULL,
  embedding vector(768), -- Gemini embedding dimension
  token_count INT NOT NULL,
  created_at TIMESTAMPTZ DEFAULT NOW()
);

-- Ãndice para busca vetorial (HNSW Ã© mais rÃ¡pido que IVFFlat)
CREATE INDEX source_chunks_embedding_idx
ON source_chunks
USING hnsw (embedding vector_cosine_ops);

-- Ãndice para ordenaÃ§Ã£o
CREATE INDEX source_chunks_source_idx
ON source_chunks(source_id, chunk_index);
```

#### **Arquivos a Criar**

**1. `supabase/functions/_shared/embeddings.ts`**

```typescript
const GEMINI_EMBEDDING_MODEL = 'gemini-embedding-001';
const CHUNK_SIZE_TOKENS = 800; // Safe limit
const CHUNK_OVERLAP_TOKENS = 100;

export interface Chunk {
  content: string;
  index: number;
  tokenCount: number;
}

export interface ChunkWithEmbedding extends Chunk {
  embedding: number[];
}

/**
 * Split text into chunks with overlap
 */
export function chunkText(text: string, chunkSize: number = CHUNK_SIZE_TOKENS): Chunk[] {
  // Rough estimate: 1 token â‰ˆ 4 characters
  const chunkSizeChars = chunkSize * 4;
  const overlapChars = CHUNK_OVERLAP_TOKENS * 4;

  const chunks: Chunk[] = [];
  let startIndex = 0;
  let chunkIndex = 0;

  while (startIndex < text.length) {
    const endIndex = Math.min(startIndex + chunkSizeChars, text.length);
    const content = text.substring(startIndex, endIndex);

    chunks.push({
      content,
      index: chunkIndex++,
      tokenCount: Math.ceil(content.length / 4)
    });

    // Move with overlap
    startIndex += chunkSizeChars - overlapChars;
  }

  return chunks;
}

/**
 * Generate embedding for a single text
 */
export async function generateEmbedding(text: string): Promise<number[]> {
  const GEMINI_API_KEY = Deno.env.get('GEMINI_API_KEY');

  if (!GEMINI_API_KEY) {
    throw new Error('GEMINI_API_KEY not configured');
  }

  const response = await fetch(
    `https://generativelanguage.googleapis.com/v1beta/models/${GEMINI_EMBEDDING_MODEL}:embedContent?key=${GEMINI_API_KEY}`,
    {
      method: 'POST',
      headers: { 'Content-Type': 'application/json' },
      body: JSON.stringify({
        content: {
          parts: [{ text }]
        }
      })
    }
  );

  if (!response.ok) {
    const error = await response.text();
    throw new Error(`Gemini Embedding API error: ${error}`);
  }

  const data = await response.json();
  return data.embedding.values;
}

/**
 * Generate embeddings for multiple chunks (batched)
 */
export async function generateEmbeddings(chunks: Chunk[]): Promise<ChunkWithEmbedding[]> {
  console.log(`ğŸ“Š [Embeddings] Generating embeddings for ${chunks.length} chunks...`);

  const chunksWithEmbeddings: ChunkWithEmbedding[] = [];

  // Process in batches of 10 to avoid rate limits
  const BATCH_SIZE = 10;

  for (let i = 0; i < chunks.length; i += BATCH_SIZE) {
    const batch = chunks.slice(i, i + BATCH_SIZE);
    const batchNum = Math.floor(i / BATCH_SIZE) + 1;
    const totalBatches = Math.ceil(chunks.length / BATCH_SIZE);

    console.log(`ğŸ”„ [Embeddings] [${batchNum}/${totalBatches}] Processing batch...`);

    const embeddings = await Promise.all(
      batch.map(chunk => generateEmbedding(chunk.content))
    );

    batch.forEach((chunk, idx) => {
      chunksWithEmbeddings.push({
        ...chunk,
        embedding: embeddings[idx]
      });
    });

    console.log(`âœ… [Embeddings] [${batchNum}/${totalBatches}] Batch complete`);
  }

  console.log(`âœ… [Embeddings] All embeddings generated successfully`);
  return chunksWithEmbeddings;
}

/**
 * Semantic search using cosine similarity
 */
export async function semanticSearch(
  supabaseClient: any,
  query: string,
  sourceIds: string[],
  topK: number = 5
): Promise<Array<{ content: string; similarity: number; sourceId: string }>> {

  // Generate embedding for query
  console.log(`ğŸ” [Search] Generating query embedding...`);
  const queryEmbedding = await generateEmbedding(query);

  // Perform vector search
  console.log(`ğŸ” [Search] Searching top ${topK} relevant chunks from ${sourceIds.length} sources...`);

  const { data, error } = await supabaseClient.rpc('match_source_chunks', {
    query_embedding: queryEmbedding,
    source_ids: sourceIds,
    match_count: topK
  });

  if (error) {
    console.error('âŒ [Search] Semantic search failed:', error);
    throw error;
  }

  console.log(`âœ… [Search] Found ${data.length} relevant chunks`);
  return data;
}
```

**2. `supabase/migrations/YYYYMMDD_add_embeddings.sql`**

```sql
-- Enable pgvector extension
CREATE EXTENSION IF NOT EXISTS vector;

-- Create source_chunks table
CREATE TABLE source_chunks (
  id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
  source_id UUID REFERENCES sources(id) ON DELETE CASCADE,
  chunk_index INT NOT NULL,
  content TEXT NOT NULL,
  embedding vector(768),
  token_count INT NOT NULL,
  created_at TIMESTAMPTZ DEFAULT NOW()
);

-- Create indexes
CREATE INDEX source_chunks_embedding_idx
ON source_chunks
USING hnsw (embedding vector_cosine_ops);

CREATE INDEX source_chunks_source_idx
ON source_chunks(source_id, chunk_index);

-- RPC function for semantic search
CREATE OR REPLACE FUNCTION match_source_chunks(
  query_embedding vector(768),
  source_ids UUID[],
  match_count INT DEFAULT 5
)
RETURNS TABLE (
  content TEXT,
  similarity FLOAT,
  source_id UUID
)
LANGUAGE plpgsql
AS $$
BEGIN
  RETURN QUERY
  SELECT
    source_chunks.content,
    1 - (source_chunks.embedding <=> query_embedding) AS similarity,
    source_chunks.source_id
  FROM source_chunks
  WHERE source_chunks.source_id = ANY(source_ids)
  ORDER BY source_chunks.embedding <=> query_embedding
  LIMIT match_count;
END;
$$;
```

**3. Nova Edge Function: `supabase/functions/generate-embeddings/index.ts`**

```typescript
import { serve } from 'https://deno.land/std@0.168.0/http/server.ts';
import { createClient } from 'https://esm.sh/@supabase/supabase-js@2';
import { authenticateRequest, createSuccessResponse, createErrorResponse } from '../_shared/security.ts';
import { chunkText, generateEmbeddings } from '../_shared/embeddings.ts';

serve(async (req) => {
  try {
    // Auth
    const authResult = await authenticateRequest(req);
    if (!authResult.authenticated || !authResult.user) {
      return new Response(JSON.stringify({ error: 'Unauthorized' }), { status: 401 });
    }

    const { source_id } = await req.json();

    const supabaseClient = createClient(
      Deno.env.get('SUPABASE_URL') ?? '',
      Deno.env.get('SUPABASE_ANON_KEY') ?? '',
      {
        global: { headers: { Authorization: req.headers.get('Authorization')! } }
      }
    );

    // Get source
    const { data: source, error: sourceError } = await supabaseClient
      .from('sources')
      .select('*')
      .eq('id', source_id)
      .single();

    if (sourceError || !source) {
      throw new Error('Source not found');
    }

    if (!source.extracted_content) {
      throw new Error('No content to embed');
    }

    console.log(`ğŸ“„ [Embeddings] Processing source: ${source.name}`);

    // 1. Chunk text
    const chunks = chunkText(source.extracted_content);
    console.log(`ğŸ“¦ [Embeddings] Split into ${chunks.length} chunks`);

    // 2. Generate embeddings
    const chunksWithEmbeddings = await generateEmbeddings(chunks);

    // 3. Store in database
    const chunksToInsert = chunksWithEmbeddings.map(chunk => ({
      source_id: source.id,
      chunk_index: chunk.index,
      content: chunk.content,
      embedding: chunk.embedding,
      token_count: chunk.tokenCount
    }));

    const { error: insertError } = await supabaseClient
      .from('source_chunks')
      .insert(chunksToInsert);

    if (insertError) throw insertError;

    console.log(`âœ… [Embeddings] Stored ${chunksWithEmbeddings.length} chunks with embeddings`);

    return createSuccessResponse({
      success: true,
      chunks_created: chunksWithEmbeddings.length
    });

  } catch (error) {
    return createErrorResponse(error as Error, 400);
  }
});
```

#### **IntegraÃ§Ã£o com Upload de PDFs**

Modificar `supabase/functions/process-pdf/index.ts` para gerar embeddings automaticamente:

```typescript
// ApÃ³s extrair conteÃºdo e salvar na tabela sources...

// Trigger embedding generation (async)
console.log('ğŸš€ Triggering embedding generation...');

await fetch(`${Deno.env.get('SUPABASE_URL')}/functions/v1/generate-embeddings`, {
  method: 'POST',
  headers: {
    'Authorization': req.headers.get('Authorization')!,
    'Content-Type': 'application/json'
  },
  body: JSON.stringify({ source_id: savedSource.id })
});
```

---

## ğŸ“‹ **FASE 3: Batching Inteligente de Output**

### **3.1 - Arquivo Compartilhado: output-limits.ts**

**Criar: `supabase/functions/_shared/output-limits.ts`**

```typescript
/**
 * Output Limits and Batching Logic
 *
 * Prevents token overflow by calculating safe batch sizes
 * based on empirical token consumption per item type.
 */

export const OUTPUT_LIMITS = {
  // Estimated tokens per item (based on testing)
  TOKENS_PER_ITEM: {
    FLASHCARD: 290,
    QUIZ_MULTIPLE_CHOICE: 400,
    QUIZ_TRUE_FALSE: 300,
    QUIZ_CLINICAL_CASE: 700,
    SUMMARY_SECTION: 1500
  },

  // Gemini API limits
  GEMINI_MAX_OUTPUT_TOKENS: 8192,

  // Safe limit (80% of max for buffer)
  SAFE_OUTPUT_LIMIT: 6400
};

export interface ValidationResult {
  valid: boolean;
  needsBatching: boolean;
  estimatedTokens: number;
  warning?: string;
  error?: string;
}

/**
 * Validate if output request is safe
 */
export function validateOutputRequest(
  itemType: keyof typeof OUTPUT_LIMITS.TOKENS_PER_ITEM,
  count: number
): ValidationResult {

  const tokensPerItem = OUTPUT_LIMITS.TOKENS_PER_ITEM[itemType];
  const estimatedTokens = count * tokensPerItem;

  // Check if it fits in single batch
  if (estimatedTokens <= OUTPUT_LIMITS.SAFE_OUTPUT_LIMIT) {
    return {
      valid: true,
      needsBatching: false,
      estimatedTokens
    };
  }

  // Check if it's within reasonable total limit (max 5 batches)
  const maxItems = Math.floor((OUTPUT_LIMITS.SAFE_OUTPUT_LIMIT * 5) / tokensPerItem);

  if (count > maxItems) {
    return {
      valid: false,
      needsBatching: false,
      estimatedTokens,
      error: `Requested ${count} items exceeds maximum of ${maxItems} items. Please reduce the count.`
    };
  }

  // Needs batching
  return {
    valid: true,
    needsBatching: true,
    estimatedTokens,
    warning: `Requested ${count} items (~${estimatedTokens} tokens) exceeds safe limit. Will process in batches.`
  };
}

/**
 * Calculate optimal batch sizes
 */
export function calculateBatchSizes(
  itemType: keyof typeof OUTPUT_LIMITS.TOKENS_PER_ITEM,
  totalCount: number
): number[] {

  const tokensPerItem = OUTPUT_LIMITS.TOKENS_PER_ITEM[itemType];
  const itemsPerBatch = Math.floor(OUTPUT_LIMITS.SAFE_OUTPUT_LIMIT / tokensPerItem);

  const batches: number[] = [];
  let remaining = totalCount;

  while (remaining > 0) {
    const batchSize = Math.min(itemsPerBatch, remaining);
    batches.push(batchSize);
    remaining -= batchSize;
  }

  return batches;
}

/**
 * Format batch progress for logging
 */
export function formatBatchProgress(current: number, total: number): string {
  return `[Batch ${current}/${total}]`;
}

/**
 * Calculate summary strategy based on input size
 */
export function calculateSummaryStrategy(inputText: string): {
  strategy: 'SINGLE' | 'BATCHED' | 'EXECUTIVE';
  explanation: string;
  estimatedOutputTokens: number;
} {

  const inputChars = inputText.length;
  const inputTokens = Math.ceil(inputChars / 4);

  // Empirical: summary is ~15-20% of input
  const estimatedOutputTokens = Math.ceil(inputTokens * 0.18);

  if (estimatedOutputTokens <= OUTPUT_LIMITS.SAFE_OUTPUT_LIMIT) {
    return {
      strategy: 'SINGLE',
      estimatedOutputTokens,
      explanation: `Input: ${inputTokens} tokens â†’ Estimated output: ${estimatedOutputTokens} tokens (fits in single request)`
    };
  }

  if (estimatedOutputTokens <= OUTPUT_LIMITS.SAFE_OUTPUT_LIMIT * 2) {
    return {
      strategy: 'BATCHED',
      estimatedOutputTokens,
      explanation: `Input: ${inputTokens} tokens â†’ Estimated output: ${estimatedOutputTokens} tokens (needs 2-3 batches)`
    };
  }

  return {
    strategy: 'EXECUTIVE',
    estimatedOutputTokens,
    explanation: `Input: ${inputTokens} tokens â†’ Output too large (${estimatedOutputTokens} tokens). Using executive summary strategy.`
  };
}
```

### **3.2 - Integrar em Todas as Edge Functions**

**Exemplo: `generate-flashcards/index.ts`**

```typescript
import { validateOutputRequest, calculateBatchSizes, formatBatchProgress, SAFE_OUTPUT_LIMIT } from '../_shared/output-limits.ts';

// ... apÃ³s validaÃ§Ã£o de entrada ...

// PHASE 1: Validate output request
const validation = validateOutputRequest('FLASHCARD', count);

if (!validation.valid) {
  return createErrorResponse(new Error(validation.error!), 400, req);
}

console.log(`ğŸ“Š [PHASE 1] Flashcard generation: ${count} cards, estimated ${validation.estimatedTokens} tokens`);

if (validation.needsBatching) {
  console.warn(`âš ï¸ [PHASE 1] ${validation.warning}`);
}

const batchSizes = calculateBatchSizes('FLASHCARD', count);
const totalBatches = batchSizes.length;

console.log(`ğŸ”„ [PHASE 1] Processing in ${totalBatches} batch(es): ${batchSizes.join(', ')} flashcards each`);

// Generate flashcards in batches
const allFlashcards: any[] = [];

for (let i = 0; i < batchSizes.length; i++) {
  const batchCount = batchSizes[i];
  const batchNum = i + 1;

  console.log(`${formatBatchProgress(batchNum, totalBatches)} Generating ${batchCount} flashcards...`);

  const prompt = `... gere ${batchCount} flashcards ...`;
  const response = await callGemini(prompt, 'gemini-2.5-flash', SAFE_OUTPUT_LIMIT);
  const parsed = parseJsonFromResponse(response);

  allFlashcards.push(...parsed.flashcards);
  console.log(`âœ… ${formatBatchProgress(batchNum, totalBatches)} Generated ${parsed.flashcards.length} flashcards`);
}

console.log(`âœ… [PHASE 1] Total flashcards generated: ${allFlashcards.length}`);
```

---

## ğŸ“‹ **FASE 4: RAG SemÃ¢ntico nas Edge Functions**

### **4.1 - Modificar Edge Functions para Usar Embeddings**

**Exemplo: `generate-flashcards/index.ts`**

```typescript
import { semanticSearch } from '../_shared/embeddings.ts';

// ... apÃ³s obter sources ...

// Check if embeddings exist
const { data: chunksExist } = await supabaseClient
  .from('source_chunks')
  .select('id')
  .in('source_id', sources.map(s => s.id))
  .limit(1);

if (!chunksExist || chunksExist.length === 0) {
  // Fallback to old method (truncated concatenation)
  console.warn('âš ï¸ No embeddings found. Using fallback method.');
  // ... cÃ³digo antigo ...
} else {
  // Use semantic search
  console.log('âœ… Using semantic search with embeddings');

  // Generate query based on task
  const query = `Criar flashcards sobre os principais conceitos mÃ©dicos, terminologia, processos fisiolÃ³gicos e patolÃ³gicos`;

  // Get top 15 most relevant chunks (limit to fit in prompt)
  const relevantChunks = await semanticSearch(
    supabaseClient,
    query,
    sources.map(s => s.id),
    15 // top K
  );

  // Build context from relevant chunks
  const combinedContent = relevantChunks
    .map((chunk, idx) => `[Chunk ${idx + 1} - RelevÃ¢ncia: ${(chunk.similarity * 100).toFixed(1)}%]\n${chunk.content}`)
    .join('\n\n---\n\n');

  console.log(`ğŸ“Š Using ${relevantChunks.length} relevant chunks (avg similarity: ${(relevantChunks.reduce((sum, c) => sum + c.similarity, 0) / relevantChunks.length * 100).toFixed(1)}%)`);
}
```

---

## ğŸ“‹ **FASE 5: Monitoramento e Observabilidade**

### **5.1 - Logs Estruturados**

**Criar: `supabase/functions/_shared/logger.ts`**

```typescript
export enum LogLevel {
  DEBUG = 'DEBUG',
  INFO = 'INFO',
  WARN = 'WARN',
  ERROR = 'ERROR'
}

export interface LogContext {
  userId?: string;
  projectId?: string;
  sourceId?: string;
  functionName: string;
  [key: string]: any;
}

export class Logger {
  constructor(private context: LogContext) {}

  private log(level: LogLevel, message: string, data?: any) {
    const logEntry = {
      timestamp: new Date().toISOString(),
      level,
      message,
      ...this.context,
      ...(data && { data })
    };

    console.log(JSON.stringify(logEntry));
  }

  debug(message: string, data?: any) {
    this.log(LogLevel.DEBUG, message, data);
  }

  info(message: string, data?: any) {
    this.log(LogLevel.INFO, message, data);
  }

  warn(message: string, data?: any) {
    this.log(LogLevel.WARN, message, data);
  }

  error(message: string, error?: Error, data?: any) {
    this.log(LogLevel.ERROR, message, {
      ...data,
      error: {
        message: error?.message,
        stack: error?.stack
      }
    });
  }
}
```

### **5.2 - MÃ©tricas de Performance**

**Adicionar em cada Edge Function:**

```typescript
const startTime = Date.now();

// ... processamento ...

const duration = Date.now() - startTime;
console.log(`â±ï¸ [Performance] Total duration: ${duration}ms`);
console.log(`ğŸ“Š [Stats] Tokens used (est): ${estimatedTokens}`);
console.log(`ğŸ“Š [Stats] Batches: ${totalBatches}`);
console.log(`ğŸ“Š [Stats] Items generated: ${result.length}`);
```

---

## ğŸ“‹ **Cronograma de ImplementaÃ§Ã£o**

| Fase | Tarefa | DuraÃ§Ã£o | Prioridade |
|------|--------|---------|------------|
| **Phase 2.1** | Schema pgvector + migration | 1h | ğŸ”´ CRÃTICA |
| **Phase 2.2** | `embeddings.ts` + semantic search | 3h | ğŸ”´ CRÃTICA |
| **Phase 2.3** | `generate-embeddings` function | 2h | ğŸ”´ CRÃTICA |
| **Phase 2.4** | Integrar com upload PDF | 1h | ğŸ”´ CRÃTICA |
| **Phase 3.1** | `output-limits.ts` completo | 2h | ğŸ”´ CRÃTICA |
| **Phase 3.2** | Integrar em flashcards | 1h | ğŸ”´ CRÃTICA |
| **Phase 3.3** | Integrar em quiz | 1h | ğŸ”´ CRÃTICA |
| **Phase 3.4** | Integrar em summary | 2h | ğŸ”´ CRÃTICA |
| **Phase 4.1** | RAG semÃ¢ntico em flashcards | 2h | ğŸŸ¡ ALTA |
| **Phase 4.2** | RAG semÃ¢ntico em quiz | 2h | ğŸŸ¡ ALTA |
| **Phase 4.3** | RAG semÃ¢ntico em summary | 2h | ğŸŸ¡ ALTA |
| **Phase 4.4** | RAG semÃ¢ntico em chat | 2h | ğŸŸ¡ ALTA |
| **Phase 5.1** | Logger estruturado | 1h | ğŸŸ¢ MÃ‰DIA |
| **Phase 5.2** | MÃ©tricas de performance | 1h | ğŸŸ¢ MÃ‰DIA |
| **Phase 6** | Testes E2E | 4h | ğŸŸ¡ ALTA |

**Total Estimado: ~27 horas (~3-4 dias de trabalho)**

---

## ğŸ¯ **PriorizaÃ§Ã£o Recomendada**

### **Sprint 1 (Dia 1-2): FundaÃ§Ã£o**
1. âœ… Schema pgvector + migration
2. âœ… `embeddings.ts` completo
3. âœ… `output-limits.ts` completo
4. âœ… `generate-embeddings` function

### **Sprint 2 (Dia 2-3): IntegraÃ§Ã£o**
1. âœ… Integrar embeddings no upload
2. âœ… Integrar output-limits em todas functions
3. âœ… RAG semÃ¢ntico em flashcards e quiz

### **Sprint 3 (Dia 3-4): Refinamento**
1. âœ… RAG semÃ¢ntico em summary e chat
2. âœ… Logger estruturado
3. âœ… Testes E2E
4. âœ… DocumentaÃ§Ã£o

---

## ğŸ“Š **Resultados Esperados**

### **Antes (Phase 0-1)**
- âŒ 5 PDFs = erro
- âŒ ConcatenaÃ§Ã£o bruta
- âŒ Contexto irrelevante no prompt
- âŒ Batching reativo

### **Depois (Phase 2-3)**
- âœ… 10+ PDFs = funciona perfeitamente
- âœ… Busca semÃ¢ntica inteligente
- âœ… Apenas chunks relevantes
- âœ… Batching preventivo
- âœ… Zero truncamento
- âœ… Logs detalhados

---

## ğŸ” **Perguntas para o UsuÃ¡rio**

Antes de comeÃ§ar a implementaÃ§Ã£o, preciso confirmar:

1. **Supabase jÃ¡ tem pgvector habilitado?** (preciso checar?)
2. **PreferÃªncia de ordem?** (comeÃ§ar por embeddings ou batching?)
3. **Quer que eu implemente tudo ou vocÃª quer fazer partes manualmente?**
4. **Limite de custos Gemini API?** (embeddings custam ~$0.00025/1K tokens)

---

## ğŸ“ **PrÃ³ximos Passos**

Aguardando sua confirmaÃ§Ã£o para comeÃ§ar. Recomendo:

**OpÃ§Ã£o A (Mais Seguro):**
1. Primeiro: Implementar `output-limits.ts` completo
2. Testar com 5 PDFs atuais
3. Depois: Implementar embeddings

**OpÃ§Ã£o B (Mais Impacto):**
1. Primeiro: Implementar embeddings
2. Testar busca semÃ¢ntica
3. Depois: Refinar batching

**Qual vocÃª prefere?** ğŸš€
